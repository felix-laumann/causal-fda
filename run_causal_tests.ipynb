{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Analysis of causal discovery algorithm on synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "from synthetic_data import generate_data\n",
    "from causal import causal_eval\n",
    "from plots import plot_SHD, plot_samples\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of trials and permutations\n",
    "n_trials = 100\n",
    "n_perms = 1000\n",
    "\n",
    "# number of samples and number of points functional data samples are (randomly) observed and discretised\n",
    "n_samples = [200]\n",
    "n_obs = 100\n",
    "n_preds = 100\n",
    "\n",
    "# define discretised mesh of points\n",
    "upper_limit = 1\n",
    "pred_points = np.linspace(0, upper_limit, n_preds)\n",
    "\n",
    "# data paramterers\n",
    "periods = [0.1]\n",
    "n_basis = 3\n",
    "sd = 1\n",
    "\n",
    "# statistical significance level\n",
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folders to save results\n",
    "if not os.path.exists('results'):\n",
    "    os.mkdir('results')\n",
    "\n",
    "if not os.path.exists('results/causal'):\n",
    "    os.mkdir('results/causal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters specific for evaluation on synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'joint'\n",
    "n_vars = 10\n",
    "prob = 0.5\n",
    "cd_type = 'combined'\n",
    "\n",
    "# historical dependence is easier to detect the higher a is\n",
    "a_list = [10, 1, 0.1]\n",
    "\n",
    "# regression parameters\n",
    "n_intervals = 12\n",
    "analyse = False\n",
    "\n",
    "# constraint parameters\n",
    "lambs = [1e-5, 1e-4, 1e-3]\n",
    "#lambs = 1e-3\n",
    "n_steps = 50\n",
    "n_pretests = 100\n",
    "\n",
    "linear = 0\n",
    "bll1, bul1, bll2, bul2, bll3, bul3, bll4, bul4 = -1, -1, -1, -1, -1, -1, -1, -1\n",
    "#bll1, bul1, bll2, bul2, bll3, bul3, bll4, bul4 = 0, 0, 0, 0, 0, 0, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Period T: 0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6ee8b6b186c43d0baf152270b4515d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 200\n",
      "a: 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7166f299ed0046bab1d95bab78aa7a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding optimal lambda:\n",
      "Evaluating lambda = 1e-05 (value 1 of 3)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcac9fac2df74eb3a5e84303f3c1b309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Completed with a rejection rate of 0.03.\n",
      "Evaluating lambda = 0.0001 (value 2 of 3)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30ac41155c5f486d8e4c84b2de658a9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Completed with a rejection rate of 0.07.\n",
      "Evaluating lambda = 0.001 (value 3 of 3)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d41cdebd99d349e4878cff6fc31d1daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Completed with a rejection rate of 0.04.\n",
      "Optimal lambda is 0.001 and resulted in a rejection rate of 0.04.\n",
      "Finding optimal lambda:\n",
      "Evaluating lambda = 1e-05 (value 1 of 3)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e2aa9806584fd58b0cc625b992c0b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Completed with a rejection rate of 0.04.\n",
      "Evaluating lambda = 0.0001 (value 2 of 3)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c940040c3944910969f6542b900c48e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Completed with a rejection rate of 100.\n",
      "Evaluating lambda = 0.001 (value 3 of 3)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5774dd17eaf448d49226ecaf2ebe7244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Completed with a rejection rate of 0.05.\n",
      "Optimal lambda is 0.001 and resulted in a rejection rate of 0.05.\n",
      "Finding optimal lambda:\n",
      "Evaluating lambda = 1e-05 (value 1 of 3)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1031dfa6f712449bb272c2873afc43fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Completed with a rejection rate of 0.07.\n",
      "Evaluating lambda = 0.0001 (value 2 of 3)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d4fbc220344088b0e9dc3883a9ae6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Completed with a rejection rate of 0.05.\n",
      "Evaluating lambda = 0.001 (value 3 of 3)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "218d322c6f824a05946c19c1b66e8e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Completed with a rejection rate of 0.07.\n",
      "Optimal lambda is 0.0001 and resulted in a rejection rate of 0.05.\n",
      "Average SHD: 23.19\n",
      "Average GL: 0.0\n",
      "----------\n",
      "a: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fed437f69334ab8a81b8185a3360510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding optimal lambda:\n",
      "Evaluating lambda = 1e-05 (value 1 of 3)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ace87ab43d41199d2936c81dd1af7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Completed with a rejection rate of 0.04.\n",
      "Evaluating lambda = 0.0001 (value 2 of 3)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd25d25e64a640478e336aa16d5ebbe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Completed with a rejection rate of 0.02.\n",
      "Evaluating lambda = 0.001 (value 3 of 3)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b29ef438f60438381f88e8fd4b98c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Completed with a rejection rate of 0.04.\n",
      "Optimal lambda is 0.001 and resulted in a rejection rate of 0.04.\n"
     ]
    }
   ],
   "source": [
    "# saving evaluation metrics\n",
    "precisions_dict = {}\n",
    "recalls_dict = {}\n",
    "f1_scores_dict = {}\n",
    "SHDs_dict = {}\n",
    "GLs_dict = {}\n",
    "averages_dict = {}\n",
    "\n",
    "# saving DAGs + p-values\n",
    "edges_dict = {}\n",
    "DAGs_dict = {}\n",
    "p_values_dict = {}\n",
    "\n",
    "for p in periods:\n",
    "    print('Period T:', p)\n",
    "    precisions_dict[p] = {}\n",
    "    recalls_dict[p] = {}\n",
    "    f1_scores_dict[p] = {}\n",
    "    SHDs_dict[p] = {}\n",
    "    GLs_dict[p] = {}\n",
    "    averages_dict[p] = {}\n",
    "    edges_dict[p] = {}\n",
    "    DAGs_dict[p] = {}\n",
    "    p_values_dict[p] = {}\n",
    "    \n",
    "    for n_sample in tqdm(n_samples):\n",
    "        print('Sample size:', n_sample)\n",
    "\n",
    "        precisions_dict[p][n_sample] = {}\n",
    "        recalls_dict[p][n_sample] = {}\n",
    "        f1_scores_dict[p][n_sample] = {}\n",
    "        SHDs_dict[p][n_sample] = {}\n",
    "        GLs_dict[p][n_sample] = {}\n",
    "        averages_dict[p][n_sample] = {}\n",
    "        edges_dict[p][n_sample] = {}\n",
    "        DAGs_dict[p][n_sample] = {}\n",
    "        p_values_dict[p][n_sample] = {}\n",
    "\n",
    "        for i, a in enumerate(a_list):\n",
    "            print('a:', a)\n",
    "\n",
    "            precisions_dict[p][n_sample][a] = []\n",
    "            recalls_dict[p][n_sample][a] = []\n",
    "            f1_scores_dict[p][n_sample][a] = []\n",
    "            SHDs_dict[p][n_sample][a] = []\n",
    "            GLs_dict[p][n_sample][a] = []\n",
    "            averages_dict[p][n_sample][a] = []\n",
    "            edges_dict[p][n_sample][a] = []\n",
    "            DAGs_dict[p][n_sample][a] = []\n",
    "            p_values_dict[p][n_sample][a] = []\n",
    "\n",
    "            # generate synthetic data\n",
    "            edges, X = generate_data(dep=test, n_samples=n_sample, n_trials=n_trials, n_obs=n_obs, n_preds=n_preds, period=p, n_vars=n_vars, a=a, upper_limit=upper_limit, n_basis=n_basis, sd=sd, prob=prob, bll1=bll1, bul1=bul1, bll2=bll2, bul2=bul2, bll3=bll3, bul3=bul3, bll4=bll4, bul4=bul4, linear=linear)\n",
    "            \n",
    "            # conduct n trials\n",
    "            precisions, recalls, f1_scores, SHDs, GLs, CPDAGs, p_values = causal_eval(cd_type=cd_type, X_dict=X, edges_dict=edges, upper_limit=upper_limit, n_preds=n_preds, n_intervals=n_intervals, n_trials=n_trials, n_perms=n_perms, alpha=alpha, K='K_ID', lambs=lambs, analyse=analyse)\n",
    "\n",
    "            precisions_dict[p][n_sample][a].append(precisions)\n",
    "            recalls_dict[p][n_sample][a].append(recalls)\n",
    "            f1_scores_dict[p][n_sample][a].append(f1_scores)\n",
    "            SHDs_dict[p][n_sample][a].append(SHDs)\n",
    "            GLs_dict[p][n_sample][a].append(GLs)\n",
    "\n",
    "            edges_dict[p][n_sample][a].append(edges)\n",
    "            DAGs_dict[p][n_sample][a].append(CPDAGs)\n",
    "            p_values_dict[p][n_sample][a].append(p_values)\n",
    "\n",
    "            # calculate average precision, recall and F1-score\n",
    "            avg_precision = np.mean(precisions_dict[p][n_sample][a])\n",
    "            avg_recall = np.mean(recalls_dict[p][n_sample][a])\n",
    "            avg_f1_score = np.mean(f1_scores_dict[p][n_sample][a])\n",
    "            avg_SHDs = np.mean(SHDs_dict[p][n_sample][a])\n",
    "            avg_GLs = np.mean(GLs_dict[p][n_sample][a])\n",
    "\n",
    "            averages_dict[p][n_sample][a].append([avg_precision, avg_recall, avg_f1_score, avg_SHDs, avg_GLs])\n",
    "\n",
    "            print('Average SHD:', avg_SHDs)\n",
    "            print('Average GL:', avg_GLs)\n",
    "            print('----------')\n",
    "        print('----------')\n",
    "    print('----------')\n",
    "\n",
    "precision_causal = open('results/causal/precision_{}_{}_{}.pkl'.format(cd_type, n_vars, n_samples[0]), 'wb')\n",
    "pickle.dump(precisions_dict, precision_causal)\n",
    "precision_causal.close()\n",
    "recall_causal = open('results/causal/recall_{}_{}_{}.pkl'.format(cd_type, n_vars, n_samples[0]), 'wb')\n",
    "pickle.dump(recalls_dict, recall_causal)\n",
    "recall_causal.close()\n",
    "f1_causal = open('results/causal/f1_{}_{}_{}.pkl'.format(cd_type, n_vars, n_samples[0]), 'wb')\n",
    "pickle.dump(f1_scores_dict, f1_causal)\n",
    "f1_causal.close()\n",
    "SHD_causal = open('results/causal/shd_{}_{}_{}.pkl'.format(cd_type, n_vars, n_samples[0]), 'wb')\n",
    "pickle.dump(SHDs_dict, SHD_causal)\n",
    "SHD_causal.close()\n",
    "GL_causal = open('results/causal/GL_{}_{}_{}.pkl'.format(cd_type, n_vars, n_samples[0]), 'wb')\n",
    "pickle.dump(GLs_dict, GL_causal)\n",
    "GL_causal.close()\n",
    "averages_causal = open('results/causal/averages_{}_{}_{}.pkl'.format(cd_type, n_vars, n_samples[0]), 'wb')\n",
    "pickle.dump(averages_dict, averages_causal)\n",
    "averages_causal.close()\n",
    "\n",
    "edges_causal = open('results/causal/edges_{}_{}_{}.pkl'.format(cd_type, n_vars, n_samples[0]), 'wb')\n",
    "pickle.dump(edges_dict, edges_causal)\n",
    "edges_causal.close()\n",
    "DAGs_causal = open('results/causal/DAGs_{}_{}_{}.pkl'.format(cd_type, n_vars, n_samples[0]), 'wb')\n",
    "pickle.dump(DAGs_dict, DAGs_causal)\n",
    "DAGs_causal.close()\n",
    "pvalues_causal = open('results/causal/p_values_{}_{}_{}.pkl'.format(cd_type, n_vars, n_samples[0]), 'wb')\n",
    "pickle.dump(p_values_dict, pvalues_causal)\n",
    "pvalues_causal.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_type = 'constraint'\n",
    "n_vars = 3\n",
    "causal_results = pickle.load(open('results/causal/shd_{}_{}_{}.pkl'.format(cd_type, n_vars, n_samples[0]), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
